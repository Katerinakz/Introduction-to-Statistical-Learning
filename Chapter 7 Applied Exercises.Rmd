---
title: "Ch 7 Applied Exercises"
author: "KK"
date: "4/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r 6}
#A
library(ISLR)
head(Wage)
library(boot)
set.seed(1)

cv.error = rep(0,10)
for (i in 1:10) {
  glm.fit=glm(wage~poly(age,i), data=Wage)
  cv.error[i]= cv.glm(Wage, glm.fit, K=10)$delta[1]
}

cv.error
plot(cv.error,type="b") 
#4 looks good! 9 looks very good too.

#ANOVA
fit.1 = lm(wage~age, data=Wage)
fit.2=lm(wage~poly(age,2), data=Wage)
fit.3=lm(wage~poly(age,3), data=Wage)
fit.4=lm(wage~poly(age,4), data=Wage)
fit.5=lm(wage~poly(age,5), data=Wage)
fit.6=lm(wage~poly(age,6), data=Wage)
fit.7=lm(wage~poly(age,7), data=Wage)
fit.8=lm(wage~poly(age,8), data=Wage)
fit.9=lm(wage~poly(age,9), data=Wage)
fit.10=lm(wage~poly(age,10), data=Wage)

anova(fit.1, fit.2, fit.3, fit.4, fit.5, fit.6, fit.7, fit.8, fit.9,
      fit.10)
#3 or 4 looks good with ANOVA.

#Let's use 4
agelims=range(age)
age.grid=seq(from=agelims[1], to=agelims[2])
preds=predict(fit.10, newdata = list(age=age.grid), se=T)
se.bands=cbind(preds$fit + 2*preds$se.fit, preds$fit-2*preds$se.fit)

par(mfrow =c(1,2) ,mar=c(4.5 ,4.5 ,1 ,1) ,oma=c(0,0,4,0))
plot(age,wage, xlim=agelims, cex=0.5, col="darkgrey")
title("Degree 4 Polynomial", outer=T)
lines(age.grid,preds$fit, lwd=2, col="blue" )
matlines(age.grid, se.bands, lwd=1, col="blue", lty=3)

#B
st.cv.error=rep(0,9)
for (i in 2:10){
  Wage$age.cut <- cut(Wage$age,i)
  glm.fit=glm(wage~age.cut, data=Wage)
  st.cv.error[i-1]= cv.glm(Wage, glm.fit, K=10)$delta[1]
}
st.cv.error
plot(2:10, st.cv.error, type="b")
#8 cuts is the best

cut.fit=lm(wage~cut(age,8), data=Wage)
cut.preds=predict(cut.fit, newdata = list(age=age.grid), se=T)
se.bands=cbind(pred$fit+2*preds$se.fit, preds$fit-2*preds$se.fit)

plot(age, wage, xlim = agelims, cex=.5, col="darkgrey")
title("Step Function- 8", outer=T)
lines(age.grid, cut.preds$fit, col="blue", lwd=2)
matlines(age.grid, se.bands, lwd=1, col="blue", lty=3)


```


```{r 7}
head(Wage)
#marit1 and jobclass
plot(Wage$maritl, Wage$wage)
plot(Wage$jobclass, Wage$wage)

library(gam)
gam.1=gam(wage~s(age,5)+maritl, data=Wage)
gam.2=gam(wage~s(age,5)+ jobclass, data=Wage)
gam.3=gam(wage~s(age,5) +jobclass+maritl, data=Wage)
anova(gam.1,gam.2, gam.3, test="F")

#From ANOVA test we see that both marital status and the job class are important along with age.

plot(gam.3, se=TRUE, col="blue")
```

```{r 8}
head(Auto)
library(boot)
library(gam)
set.seed(1)

cv.error=rep(0,10)
for (i in 1:10){
  fit.auto=glm(mpg~poly(horsepower,i), data=Auto)
  cv.error[i]=cv.glm(Wage, glm.fit, K=10)$delta[1]
}
cv.error
plot(cv.error, type="b")

#Let's try GAM with adding cylinders and weight
gam1=gam(mpg~s(horsepower,7)+weight, data=Auto)
gam2=gam(mpg~s(horsepower,7) +cylinders, data=Auto)
gam3=gam(mpg~s(horsepower,7) +cylinders+ weight, data=Auto)
anova(gam1, gam2, gam3, test="F")
#Both weight and cylinders are significant in predicting mpg.
plot(gam3, se=TRUE, col="blue")
```


```{r 9}
#A
head(Boston)
glm.fit3= lm(nox~poly(dis,3), data=Boston)
dislims=range(Boston$dis)
dis.grids=seq(from=dislims[1], to=dislims[2], 0.1)
preds.fit3= predict(glm.fit3, newdata = list(dis=dis.grids), 
                    se=TRUE)
se.bands.fit3=cbind(preds.fit3$fit+2*preds.fit3$se.fit)

plot(Boston$dis, Boston$nox ,cex=.5, col="darkgrey")
title("Degree-3 Polynomial", outer=T)
lines(dis.grids, preds.fit3$fit, lwd=2, col="blue")
matlines(dis.grids, se.bands.fit3, lwd=2, col="blue", lty=3)


#B
rss.err=rep(0,10)
for (i in 1:10){
  glm.fit=glm(nox~poly(dis,i), data=Boston)
  rss.err[i]=sum(glm.fit$residuals^2)
}
rss.err
plot(rss.err, type="b")

#C
cv.errors=rep(0,10)
for (i in 1:10){
  glm.fit=glm(nox~poly(dis,i), data=Boston)
  cv.errors[i]=cv.glm(Boston, glm.fit, K=10)$delta[1]
}
cv.errors
plot(cv.errors, type="b")
#the best option is on the 4th degree, definitely do not recommend above 6 degree.

#D
library(splines)
fit.splin=lm(nox~bs(dis, df=4), data=Boston)
pred.splin=predict(fit.splin, newdata=list(dis=dis.grids), se=T)

plot(Boston$dis, Boston$nox, col="darkgrey")
lines(dis.grids, pred.splin$fit, lwd=2)
lines(dis.grids,pred.splin$fit+2* pred.splin$se, lty="dashed")
lines(dis.grids,pred.splin$fit-2* pred.splin$se, lty="dashed")

attr(bs(Boston$dis,df=4),"knots")
#only one knot at 50%

#E
rss.error=rep(0,7)
for (i in 4:10){
  fit.spl=lm(nox~bs(dis, df=i), data=Boston)
  rss.error[i-3]=sum(fit.spl$residuals^2)
}
rss.error
plot(rss.error, type="b") 
#RSS diminishes as the degree of freedom increases in the training data.

#F
library(boot)
set.seed(1)
cv.error <- rep(0,7)
for (i in 4:10) {
  glm.fit <- glm(nox~bs(dis, df=i), data=Boston)
  cv.error[i-3] <- cv.glm(Boston, glm.fit, K=10)$delta[1]
}
cv.error
plot(4:10, cv.error, type="b")
#should be at least df 5

```

```{r 10}
head(College)
#A
trainid=sample(1:nrow(College), nrow(College)/2)
train=College[trainid,]
test=College[-trainid,]

library(leaps)
fwd.reg=regsubsets(Outstate~. , data=train, nvmax=ncol(College)-1, method="forward")
summ.fwd=summary(fwd.reg)

fwd.err=rep(NA, ncol(College)-1)
for (i in 1:(ncol(College)-1)){
  pred.fwd=predict(fwd.reg, test, id=i)
  fwd.err[i]=mean((test$Outstate-pred.fwd)^2)
}

plot(fwd.err, type="b", main="Test MSE", xlab="Number of Predictors") #6 has the lowest test MSE

min.cp=which.min(summ.fwd$cp) #12 is the lowest
plot(summ.fwd$cp, type="b", main="CP", xlab="Number of Predictors")
points(min.cp, summ.fwd$cp[min.cp], col="red", pch=4, lwd=5)

max.adjr=which.max(summ.fwd$adjr2) #13 is the highest
plot(summ.fwd$adjr2, type="b", main="ADJ R^2", xlab="Number of Predictors")
points(max.adjr, summ.fwd$adjr2[max.adjr], col="red", pch=4, lwd=5)

min.bic=which.min(summ.fwd$bic) # 9 is the lowest
plot(summ.fwd$bic, type="b", main="BIC", xlab="Number of Predictors")
points(min.bic, summ.fwd$bic[min.bic], col="red", pch=4, lwd=5)

#Since 6 predictors has the lowest MSE and the metrics do not improve much after 6 preictors, we will choose the 6 predictors to run GAM in the next question. 
coef(fwd.reg,6)

#B
library(gam)
gam.coll=gam(Outstate~ Private + 
               s(Room.Board,2) +
               s(Terminal,2)+
               s(perc.alumni,3)+
               s(Expend,4)+
               s(Grad.Rate,4), data=train)
plot(gam.coll, se=T, col="green")

#C
pred.gam=predict(gam.coll, newdata=test)
mse.err=sum((test$Outstate-pred.gam)^2)
mse.err
fwd.err[6] #the linear fit is working better in this case than the GAM. 

#D
summary(gam.coll)
#The summary shows that there is evidence of the Expend variable having a non linear relationship with the response.

```


```{r 11}
#A
set.seed(10)
X1=rnorm(100)
X2=rnorm(100)
Y=3.2 + 1.1*X1 +0.7*X2

#B
beta0=rep(NA, 1000)
beta1=rep(NA,1000)
beta2=rep(NA,1000)
beta1=5

#C
a=Y-beta1*X1
beta2=lm(a~X2)$coef[2]

#D
beta2=1.3
a=Y-beta2*X2
beta1=lm(a~X2)$coef[2]

#E
for (i in 1:1000){
  a=Y-beta1[i]*X1
  beta2[i]=lm(a~X2)$coef[2]
  a=Y-beta2[i]*X2
  beta0[i]=lm(a~X1)$coef[1]
  beta1[i+1]=lm(a~X1)$coef[2]
}
library(ggplot2)
library(reshape2)

df <- data.frame(Iteration=1:1000, beta0, beta1=beta1[-1], beta2)
df2 <- melt(df, id.vars="Iteration")
ggplot(df2, aes(x=Iteration, y=value, group=variable, col=variable)) + 
  geom_line(size=1) + ggtitle("Plot of beta estimates by Iteration")

#F
fit.lm=lm(Y~X1+X2)
coef(fit.lm)
#they match exactly

#G
head(df2)
#At the third backfitting iteration, we got the right prediction.
```












